/******************************************************************************
 *  Copyright (c) 2019, Xilinx, Inc.
 *  All rights reserved.
 *
 *  Redistribution and use in source and binary forms, with or without
 *  modification, are permitted provided that the following conditions are met:
 *
 *  1.  Redistributions of source code must retain the above copyright notice,
 *     this list of conditions and the following disclaimer.
 *
 *  2.  Redistributions in binary form must reproduce the above copyright
 *      notice, this list of conditions and the following disclaimer in the
 *      documentation and/or other materials provided with the distribution.
 *
 *  3.  Neither the name of the copyright holder nor the names of its
 *      contributors may be used to endorse or promote products derived from
 *      this software without specific prior written permission.
 *
 *  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 *  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
 *  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 *  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR
 *  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 *  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 *  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
 *  OR BUSINESS INTERRUPTION). HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
 *  WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
 *  OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
 *  ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 *******************************************************************************/

/*******************************************************************************
 *
 *  Authors: Giulio Gambardella <giuliog@xilinx.com>
 *           Thomas B. Preusser <thomas.preusser@utexas.edu>
 *             Marie-Curie Fellow, Xilinx Ireland, Grant Agreement No. 751339
 *           Christoph Doehring <cdoehrin@xilinx.com>
 *
 *  \file mvau.hpp
 *
 *  This file lists a templated funtion used to implement  
 *  Matrix-Vector-Activation Unit
 *
 *  This project has received funding from the European Union's Framework
 *  Programme for Research and Innovation Horizon 2020 (2014-2020) under
 *  the Marie Skłodowska-Curie Grant Agreement No. 751339.
 *
 *******************************************************************************/

#ifndef MVAU_HPP
#define MVAU_HPP

#include "hls_stream.h"

#include "mac.hpp"
#include "interpret.hpp"
#include "weights.hpp"

/**
 * \brief Matrix vector activate function
 *
 * The function performs the multiplication between a weigth matrix and the input activation vector,
 * accumulating the results and then applying an activation function on the accumulated result.
 *
 * 
 * \tparam MatrixW    Width of the input matrix
 * \tparam MatrixH    Heigth of the input matrix
 * \tparam SIMD       Number of input columns computed in parallel
 * \tparam PE         Number of output rows computed in parallel
 * \tparam MMV        Number of output pixels computed in parallel
 * \tparam TSrcI      DataType of the input activation (as used in the MAC)
 * \tparam TDstI      DataType of the output activation (as generated by the activation)
 * \tparam TWeightI   DataType of the weights and how to access them in the array
 * \tparam TI         DataType of the input stream - safely deducible from the paramaters
 * \tparam TO         DataType of the output stream - safely deducible from the paramaters
 * \tparam TW         DataType of the weights matrix - safely deducible from the paramaters
 * \tparam TA         DataType of the activation class (e.g. thresholds) - safely deducible from the paramaters
 * \tparam R          Datatype for the resource used for FPGA implementation of the MAC  - safely deducible from the paramaters
 *
 * \param in          Input stream
 * \param out         Output stream
 * \param weights     Weights matrix (currently supports BinaryWeights or FixedPointWeights)
 * \param activation  Activation class
 * \param reps        Number of time the function has to be repeatedly executed (e.g. number of images)
 * \param r           Resource type for the hardware implementation of the MAC block
 */
template<
  unsigned MatrixW, unsigned MatrixH, unsigned SIMD, unsigned PE, unsigned MMV, 
  typename TSrcI = Identity, typename TDstI = Identity, typename TWeightI = Identity,
  typename TI, typename TO, typename TW, typename TA, typename R
>
void Matrix_Vector_Activate_Batch(hls::stream<TI> &in,
				  hls::stream<TO> &out,
				  TW  const &weights,
				  TA  const &activation,
				  int const  reps,
				  R const &r) {

  // how many different rows each neuron will compute
  // alternatively: number of vertical matrix chunks
  unsigned const  NF = MatrixH / PE;

  // how many synapse groups each row is split into
  // alternatively: number of horizontal matrix chunks
  unsigned const  SF = MatrixW / SIMD;

  // input vector buffers
  TI  inputBuf[SF];
#pragma HLS ARRAY_PARTITION variable=inputBuf complete dim=0


  decltype(activation.init(0,0))  accu[MMV][PE];
#pragma HLS ARRAY_PARTITION variable=accu complete dim=0

  unsigned  nf   = 0;
  unsigned  sf   = 0;
  unsigned  tile = 0; // invariant: tile = nf*SF + sf

  // everything merged into a common iteration space (one "big" loop instead
  // of smaller nested loops) to get the pipelinening the way we want
  unsigned const TOTAL_FOLD = NF * SF;
  for(unsigned  i = 0; i < reps * TOTAL_FOLD; i++) {
#pragma HLS pipeline style=flp II=1
    TI  inElem;
    if(nf == 0) {
      // read input from stream
      inElem = in.read();
      // store in appropriate buffer for reuse
      inputBuf[sf] = inElem;
    }
    else {
      // reuse buffered input
      inElem = inputBuf[sf];
    }

    // Threshold Initialisation
    if(sf == 0) {
      for(unsigned  pe = 0; pe < PE; pe++) {
#pragma HLS UNROLL
        for(unsigned mmv = 0; mmv < MMV; mmv++) {
#pragma HLS UNROLL
          accu[mmv][pe] = activation.init(nf, pe);
        }
      }
    }

    // compute matrix-vector product for each processing element
    auto const &w = weights.weights(tile);
    for(unsigned  pe = 0; pe < PE; pe++) {
#pragma HLS UNROLL
      auto const  wgt = TWeightI()(w[pe]);
      for (unsigned mmv = 0; mmv < MMV; mmv++){
        auto const  act = TSrcI()(inElem, mmv);
        accu[mmv][pe] = mac<SIMD>(accu[mmv][pe], wgt, act, r, mmv);
      }
    }

    // keep track of which folded synapse/neuron we are processing
    ++tile;
    if(++sf == SF) {
      // produce output and clear accumulators
      auto  outElem = TDstI().template operator()<TO>();
      for (unsigned  pe = 0; pe < PE; pe++) {
#pragma HLS UNROLL
        for (unsigned mmv = 0; mmv < MMV; mmv++){
#pragma HLS UNROLL
          outElem(pe,mmv,1) = activation.activate(nf, pe, accu[mmv][pe]);
        }
      }
      out.write(outElem);
      // next folded neuron or image
      sf = 0;
      if(++nf == NF) {
	    nf   = 0;
	    tile = 0;
      }
    }
  }
}


/**
 * \brief Matrix vector activate function with streaming weights
 *
 * The function performs the multiplication between a weigth matrix, presnted as an input stream, and the input activation vector,
 * accumulating the results and then applying an activation function on the accumulated result. Does not support MMV.
 *
 * 
 * \tparam MatrixW    Width of the input matrix
 * \tparam MatrixH    Heigth of the input matrix
 * \tparam SIMD       Number of input columns computed in parallel
 * \tparam PE         Number of output rows computed in parallel
 * \tparam TSrcI      DataType of the input activation (as used in the MAC)
 * \tparam TDstI      DataType of the output activation (as generated by the activation)
 * \tparam TWeightI   DataType of the weights and how to access them in the array
 * \tparam TW         DataType of the weights (as used in the MAC) - not deducible from the paramaters
 * \tparam TI         DataType of the input stream - safely deducible from the paramaters
 * \tparam TO         DataType of the output stream - safely deducible from the paramaters
 * \tparam TA         DataType of the activation class (e.g. thresholds) - safely deducible from the paramaters
 * \tparam R          Datatype for the resource used for FPGA implementation of the MAC  - safely deducible from the paramaters
 *
 * \param in          Input stream
 * \param out         Output stream
 * \param weight      Weight stream (currently supports BinaryWeights or FixedPointWeights)
 * \param activation  Activation class
 * \param reps        Number of time the function has to be repeatedly executed (e.g. number of images)
 * \param r           Resource type for the hardware implementation of the MAC block
 */
template<
  unsigned MatrixW, unsigned MatrixH, unsigned SIMD, unsigned PE, 
  typename TSrcI = Identity, typename TDstI = Identity, typename TWeightI = Identity, typename TW,
  typename TI, typename TO, typename TA, typename R
>
void Matrix_Vector_Activate_Stream_Batch(hls::stream<TI> &in,
          hls::stream<TO> &out,
          hls::stream<ap_uint<PE*SIMD*TW::width>> &weight,
          TA  const &activation,
          int const  reps,
          R const &r) {

  // how many different rows each neuron will compute
  // alternatively: number of vertical matrix chunks
  unsigned const  NF = MatrixH / PE;

  // how many synapse groups each row is split into
  // alternatively: number of horizontal matrix chunks
  unsigned const  SF = MatrixW / SIMD;

  // input vector buffers
  TI  inputBuf[SF];
#pragma HLS ARRAY_PARTITION variable=inputBuf complete dim=1
  // accumulators
  decltype(activation.init(0,0))  accu[1][PE];
#pragma HLS ARRAY_PARTITION variable=accu complete dim=0
  // unpacked and packed buffers for weight stream
  Weights_Tile<SIMD, TW, PE > w;
#pragma HLS ARRAY_PARTITION variable=w.m_weights complete dim=0
  ap_uint<PE * SIMD * TW::width> W_packed;


  unsigned  nf   = 0;
  unsigned  sf   = 0;
  unsigned  tile = 0; // invariant: tile = nf*SF + sf
  
  // everything merged into a common iteration space (one "big" loop instead
  // of smaller nested loops) to get the pipelinening the way we want
  unsigned const TOTAL_FOLD = NF * SF;
  for(unsigned  i = 0; i < reps * TOTAL_FOLD; i++) {
#pragma HLS pipeline style=flp II=1
    TI  inElem;

    if(nf == 0) {
      // read input from stream
      inElem = in.read();
      // store in appropriate buffer for reuse
      inputBuf[sf] = inElem;
    }
    else {
      // reuse buffered input
      inElem = inputBuf[sf];
    }

    // read from the parameter stream
    W_packed = weight.read();
    for (unsigned pe = 0; pe < PE; pe++) {
#pragma HLS UNROLL
      w.m_weights[pe] = W_packed((pe+1)*SIMD*TW::width-1,pe*SIMD*TW::width);
    }

    // Threshold Initialisation
    if(sf == 0) {
      for(unsigned pe = 0; pe < PE; pe++) {
#pragma HLS UNROLL
      accu[0][pe] = activation.init(nf, pe);
      }
    }

    // compute matrix-vector product for each processing element
    for(unsigned  pe = 0; pe < PE; pe++) {
#pragma HLS UNROLL
      auto const  act = TSrcI()(inElem, 0);
      auto const  wgt = TWeightI()(w[pe]);
      //auto const  wgt = w[pe];
      accu[0][pe] = mac<SIMD>(accu[0][pe], wgt, act, r, 0);
    }

    // keep track of which folded synapse/neuron we are processing
    ++tile;
    if(++sf == SF) {
      // produce output and clear accumulators
      auto  outElem = TDstI().template operator()<TO>();
      for (unsigned  pe = 0; pe < PE; pe++) {
#pragma HLS UNROLL
      outElem(pe,0,1) = activation.activate(nf, pe, accu[0][pe]);
          }

      out.write(outElem);

      // next folded neuron or image
      sf = 0;
      if(++nf == NF) {
      nf   = 0;
      tile = 0;
      }
    }
  }
}

// /**
//  * \brief Matrix vector activate function with streaming weights + block-sparse tile skipping
//  *
//  * 新增：通过 tile_nz（1-bit/ tile）指示是否需要从权重流读取并进行 MAC；为 0 时直接跳过本 tile。
//  *
//  * \tparam MatrixW    Width of the input matrix
//  * \tparam MatrixH    Heigth of the input matrix
//  * \tparam SIMD       Number of input columns computed in parallel
//  * \tparam PE         Number of output rows computed in parallel
//  * \tparam TSrcI      DataType of the input activation (as used in the MAC)
//  * \tparam TDstI      DataType of the output activation (as generated by the activation)
//  * \tparam TWeightI   DataType of the weights and how to access them in the array
//  * \tparam TW         DataType of the weights (as used in the MAC)
//  * \tparam TI         DataType of the input stream
//  * \tparam TO         DataType of the output stream
//  * \tparam TA         DataType of the activation class (e.g. thresholds)
//  * \tparam R          Datatype for the resource used for FPGA implementation of the MAC
//  *
//  * \param in          Input stream
//  * \param out         Output stream
//  * \param weight      Weight stream (only为 tile_nz=1 的 tile 提供数据；1 个 tile -> 1 个打包字)
//  * \param tile_nz     稀疏标志流：每 tile 一个比特（1=非零tile，0=全零tile），顺序与内部遍历一致
//  * \param activation  Activation class
//  * \param reps        Number of images/batches
//  * \param r           Resource type for the hardware implementation of the MAC block
//  */
// template<
//   unsigned MatrixW, unsigned MatrixH, unsigned SIMD, unsigned PE, 
//   typename TSrcI = Identity, typename TDstI = Identity, typename TWeightI = Identity, typename TW,
//   typename TI, typename TO, typename TA, typename R
// >
// void Matrix_Vector_Activate_Stream_Batch_sparse(
//           hls::stream<TI> &in,
//           hls::stream<TO> &out,
//           hls::stream<ap_uint<PE*SIMD*TW::width>> &weight,
//           hls::stream<ap_uint<1>> &tile_nz,   // ★ 新增：tile 稀疏标志流
//           TA  const &activation,
//           int const  reps,
//           R const &r) {

//   // 编译期健壮性检查
//   static_assert(MatrixW % SIMD == 0, "MatrixW must be divisible by SIMD");
//   static_assert(MatrixH % PE   == 0, "MatrixH must be divisible by PE");

//   // how many different rows each neuron will compute (vertical chunks)
//   unsigned const  NF = MatrixH / PE;

//   // how many synapse groups each row is split into (horizontal chunks)
//   unsigned const  SF = MatrixW / SIMD;

//   // input vector buffers
//   TI  inputBuf[SF];
// #pragma HLS ARRAY_PARTITION variable=inputBuf complete dim=1

//   // accumulators
//   decltype(activation.init(0,0))  accu[1][PE];
// #pragma HLS ARRAY_PARTITION variable=accu complete dim=0

//   // unpacked and packed buffers for weight stream
//   Weights_Tile<SIMD, TW, PE > w;
// #pragma HLS ARRAY_PARTITION variable=w.m_weights complete dim=0
//   ap_uint<PE * SIMD * TW::width> W_packed;

//   unsigned  nf   = 0;
//   unsigned  sf   = 0;
//   unsigned  tile = 0; // invariant: tile = nf*SF + sf

//   unsigned const TOTAL_FOLD = NF * SF;

//   for (unsigned i = 0; i < (unsigned)reps * TOTAL_FOLD; i++) {
// #pragma HLS pipeline style=flp II=1

//     TI inElem;

//     // 读取/复用输入（与原逻辑一致）
//     if (nf == 0) {
//       inElem = in.read();
//       inputBuf[sf] = inElem;
//     } else {
//       inElem = inputBuf[sf];
//     }

//     // 每个 tile 先读 1bit 稀疏标志
//     ap_uint<1> nz = tile_nz.read();

//     // 仅当该 tile 非零时，才从权重流读取并解包
//     if (nz) {
//       W_packed = weight.read();
//       for (unsigned pe = 0; pe < PE; pe++) {
// #pragma HLS UNROLL
//         w.m_weights[pe] =
//           W_packed((pe+1)*SIMD*TW::width - 1, pe*SIMD*TW::width);
//       }
//     }

//     // Threshold Initialisation（每个 nf 开始时初始化累加器）
//     if (sf == 0) {
//       for (unsigned pe = 0; pe < PE; pe++) {
// #pragma HLS UNROLL
//         accu[0][pe] = activation.init(nf, pe);
//       }
//     }

//     // 仅当该 tile 非零时进行 MAC；为零则等价于加 0（直接跳过）
//     if (nz) {
//       for (unsigned pe = 0; pe < PE; pe++) {
// #pragma HLS UNROLL
//         auto const act = TSrcI()(inElem, 0);
//         auto const wgt = TWeightI()(w[pe]);
//         accu[0][pe] = mac<SIMD>(accu[0][pe], wgt, act, r, 0);
//       }
//     }

//     // 维护 tile/nf/sf 游标与输出（与原逻辑一致）
//     ++tile;
//     if (++sf == SF) {
//       // 生成输出
//       auto outElem = TDstI().template operator()<TO>();
//       for (unsigned pe = 0; pe < PE; pe++) {
// #pragma HLS UNROLL
//         outElem(pe, 0, 1) = activation.activate(nf, pe, accu[0][pe]);
//       }
//       out.write(outElem);

//       sf = 0;
//       if (++nf == NF) {
//         nf   = 0;
//         tile = 0;
//       }
//     }
//   }
// }


// /**
//  * \brief Matrix vector activate function with CSR streaming weights
//  *
//  * CSR-style streaming:
//  *  - rowlen_stream: for every PE-group (i.e. NF times) we get PE row lengths
//  *  - colidx_stream: for every "sparse tile" we get PE*SIMD column indices
//  *  - val_stream:    for every "sparse tile" we get PE*SIMD weight values
//  *
//  * Producer rule (important):
//  *  For each PE-group, producer must send exactly `max_row_len_in_group` sparse tiles,
//  *  where each sparse tile holds up to SIMD nonzeros per row (packed to PE*SIMD),
//  *  and "unused" positions are don't-care (consumer will ignore them with t*SIMD+s < row_len[pe]).
//  */
// template<
//   unsigned MatrixW,             // #cols
//   unsigned MatrixH,             // #rows
//   unsigned SIMD,                // parallel columns
//   unsigned PE,                  // parallel rows
//   unsigned ColIdxWidth,         // bits to encode column index: ceil(log2(MatrixW))
//   typename TSrcI  = Identity,
//   typename TDstI  = Identity,
//   typename TWeightI = Identity, // how to unpack weight scalar
//   typename TI,                  // input activation stream type (packed)
//   typename TO,                  // output activation stream type (packed)
//   typename TA,                  // activation class
//   typename TW,                  // weight scalar storage type, must have TW::width
//   typename R                    // MAC resource selector
// >
// void Matrix_Vector_Activate_CSR_Stream_Batch(
//     hls::stream<TI> &in,                                           // activation vector (dense)
//     hls::stream<ap_uint<PE*SIMD*ColIdxWidth>> &colidx_stream,      // sparse column indices
//     hls::stream<ap_uint<PE*SIMD*TW::width>>     &val_stream,       // sparse values
//     hls::stream<ap_uint<PE*16>>                 &rowlen_stream,    // #nonzeros per row (per PE)
//     TA  const  &activation,
//     int const   reps,
//     R   const  &r)
// {
//   // how many input tiles form ONE dense input vector
//   static const unsigned SF = MatrixW / SIMD; // assumed divisible
//   // how many groups of PE-rows form the full matrix
//   static const unsigned NF = MatrixH / PE;   // assumed divisible

//   // dense activation buffer: we keep it tile-wise (like your original version)
//   TI actBuf[SF];
// #pragma HLS ARRAY_PARTITION variable=actBuf complete dim=1

//   // per-PE accumulators
//   decltype(activation.init(0,0)) accu[PE];
// #pragma HLS ARRAY_PARTITION variable=accu complete dim=0

//   // main repetition loop (e.g. #images)
//   for (int rep = 0; rep < reps; ++rep) {

//     //
//     // 1) read the whole input activation vector ONCE and buffer it
//     //
//     for (unsigned sf = 0; sf < SF; ++sf) {
// #pragma HLS PIPELINE II=1
//       TI inElem = in.read();
//       actBuf[sf] = inElem;
//     }

//     //
//     // 2) go over matrix rows in groups of PE
//     //
//     for (unsigned nf = 0; nf < NF; ++nf) {

//       // 2.1) read PE row lengths (how many nonzeros in each of the PE rows)
//       ap_uint<PE*16> packed_len = rowlen_stream.read();

//       unsigned row_len[PE];
// #pragma HLS ARRAY_PARTITION variable=row_len complete dim=0

//       unsigned max_len = 0; // in units of "SIMD nonzero slots"
//       for (unsigned pe = 0; pe < PE; ++pe) {
// #pragma HLS UNROLL
//         // each row length is in actual nonzeros, not SIMD-chunks
//         unsigned rl = packed_len.range((pe+1)*16-1, pe*16);
//         row_len[pe] = rl;
//         if (rl > max_len)
//           max_len = rl;
//         // init accumulator for this row
//         accu[pe] = activation.init(nf, pe);
//       }

//       // 2.2) we process as many sparse tiles as the longest row in this PE group needs
//       // each iteration consumes *one* sparse-tile from the two streams
//       // (colidx_stream and val_stream)
//       const unsigned tiles_needed = (max_len + SIMD - 1) / SIMD; // round up to SIMD
//       for (unsigned t = 0; t < tiles_needed; ++t) {
// #pragma HLS PIPELINE II=1

//         // get PE*SIMD column indices and values for THIS sparse-tile
//         ap_uint<PE*SIMD*ColIdxWidth> packed_idx = colidx_stream.read();
//         ap_uint<PE*SIMD*TW::width>   packed_val = val_stream.read();

//         // for every PE row in this group, handle its active lanes
//         for (unsigned pe = 0; pe < PE; ++pe) {
// #pragma HLS UNROLL
//           // how many nonzeros of this row are *still* not consumed
//           const unsigned remaining = row_len[pe] > t*SIMD ? (row_len[pe] - t*SIMD) : 0;
//           // we can consume at most SIMD of them in this tile
//           const unsigned lanes_this_time = (remaining > SIMD) ? SIMD : remaining;

//           // loop over SIMD lanes
//           for (unsigned s = 0; s < SIMD; ++s) {
// #pragma HLS UNROLL
//             if (s < lanes_this_time) {
//               // 1) extract column index for (pe,s)
//               const unsigned idx_lo = (pe*SIMD + s) * ColIdxWidth;
//               const unsigned idx_hi = idx_lo + ColIdxWidth - 1;
//               ap_uint<ColIdxWidth> col = packed_idx.range(idx_hi, idx_lo);

//               // 2) fetch activation element: we stored it tile-wise
//               unsigned sf  = col / SIMD;
//               unsigned lan = col % SIMD;
//               TI inElem = actBuf[sf];
//               auto act = TSrcI()(inElem, lan);

//               // 3) extract weight value for (pe,s)
//               const unsigned val_lo = (pe*SIMD + s) * TW::width;
//               const unsigned val_hi = val_lo + TW::width - 1;
//               ap_uint<TW::width> wbits = packed_val.range(val_hi, val_lo);
//               auto wgt = TWeightI()(wbits);

//               // 4) MAC – note we already unrolled SIMD, so mac<1>
//               accu[pe] = mac<1>(accu[pe], wgt, act, r, 0);
//             }
//           } // s
//         } // pe
//       } // t

//       // 2.3) produce PE outputs for this group of rows
//       auto outElem = TDstI().template operator()<TO>();
//       for (unsigned pe = 0; pe < PE; ++pe) {
// #pragma HLS UNROLL
//         outElem(pe, 0, 1) = activation.activate(nf, pe, accu[pe]);
//       }
//       out.write(outElem);
//     } // nf
//   } // reps
// }


#endif
